{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de267399-0f3d-4478-a186-798899863d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8769c93-e4c0-4df9-81ce-327d31845ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "desktop_path = os.path.expanduser(\"~/Desktop\")  # Gets the desktop path\n",
    "directory = os.path.join(desktop_path, \"Plantas\", \"PlantVillage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa83e6-5e30-4cbc-a34c-cad62809cf53",
   "metadata": {},
   "source": [
    "Cargar y preprocesar las im√°genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883759b-5ae2-4204-857f-01f01bedf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Normalizaci√≥n [0, 1]\n",
    "    rotation_range=40,       # Rotaci√≥n aleatoria\n",
    "    width_shift_range=0.2,   # Desplazamiento horizontal\n",
    "    height_shift_range=0.2,  # Desplazamiento vertical\n",
    "    shear_range=0.2,         # Deformaci√≥n\n",
    "    zoom_range=0.2,          # Zoom aleatorio\n",
    "    horizontal_flip=True,    # Volteo horizontal\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generadores de datos\n",
    "batch_size = 32\n",
    "img_size = (224, 224)  # ResNet50 espera 224x224 por defecto\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=directory,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=directory,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6bc988-8fa1-4b93-b8a5-b8be987f3d7c",
   "metadata": {},
   "source": [
    "Verificar las clases cargadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909c758-10c7-4239-a6ca-db55330d7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clases encontradas:\", train_generator.class_indices)\n",
    "print(\"N√∫mero de clases:\", train_generator.num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f425c-4f97-4c74-968b-ab3279b3b70a",
   "metadata": {},
   "source": [
    "Visualizar im√°genes del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b13d6-a38f-4064-becb-537001ebeaa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(x_batch[i])\n",
    "    plt.title(np.argmax(y_batch[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eecd95-1785-436a-a559-32e262ada696",
   "metadata": {},
   "source": [
    "\n",
    "C√≥digo base: loop de experimentos entrenamos 5‚ÄØ√©pocas en feature‚Äëextraction (capas congeladas), guardamos el mejor val_accuracy, liberamos memoria y seguimos. M√°s adelante afinamos el ganador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466961f2-f05a-4d48-88c0-bedb3accf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, gc, pandas as pd, time\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2, EfficientNetB0, ResNet50, InceptionV3, DenseNet121\n",
    ")\n",
    "\n",
    "IMG_SIZE = (224, 224); BATCH = 32; EPOCHS = 5\n",
    "BACKBONES = {\n",
    "    \"MobileNetV2\": MobileNetV2,\n",
    "    \"EfficientNetB0\": EfficientNetB0,\n",
    "    \"ResNet50\": ResNet50,\n",
    "    \"InceptionV3\": InceptionV3,\n",
    "    \"DenseNet121\": DenseNet121\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, constructor in BACKBONES.items():\n",
    "    print(f\"\\nüîÑ Entrenando {name}\")\n",
    "    base = constructor(weights=\"imagenet\", include_top=False,\n",
    "                       input_shape=IMG_SIZE + (3,))\n",
    "    base.trainable = False\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(train_generator.num_classes, activation=\"softmax\")(x)\n",
    "    model = models.Model(base.input, out)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    hist = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "    t_train = time.time() - t0\n",
    "    \n",
    "    best_val = max(hist.history[\"val_accuracy\"])\n",
    "    results.append({\"model\": name, \"val_acc\": best_val, \"train_time_s\": t_train})\n",
    "    \n",
    "    # liberar memoria GPU/CPU\n",
    "    del model, base, hist; gc.collect(); tf.keras.backend.clear_session()\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"val_acc\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc210d0-504a-4a38-b484-ea6afa127298",
   "metadata": {},
   "source": [
    "\n",
    "Usar EarlyStopping(patience=2, restore_best_weights=True) para evitar sobreajuste incluso en estas 5‚ÄØ√©pocas.\n",
    "\n",
    "Se puede reducir IMG_SIZE a 160‚ÄØ√ó‚ÄØ160 para acelerar las redes grandes en Colab Free.\n",
    "\n",
    "Elegir el mejor modelo y fine‚Äëtuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7e195-5c70-4c13-9789-28063295a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = \"MobileNetV2\"\n",
    "\n",
    "base = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "                      input_shape=IMG_SIZE + (3,))\n",
    "# 1) Feature extraction head de nuevo\n",
    "...\n",
    "model = ...\n",
    "# 2) Cargar pesos previos si los guardaste (opcional)\n",
    "# model.load_weights(\"efficientnet_b0_feature_ext.h5\")\n",
    "\n",
    "# 3) Descongelar √∫ltimas N capas\n",
    "for layer in base.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # lr bajo para fine‚Äëtune\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "ft_hist = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "model.save(\"MobileNetV2_finetuned.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27603a-bfff-49df-ab72-04ab7b36ab8f",
   "metadata": {},
   "source": [
    "\n",
    "Evaluaci√≥n final y matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8be530-adb8-4676-800f-21458a642762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns; import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generador de test (sin augmentations)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory=directory,\n",
    "    subset=None,  # si lo separaste en /test/\n",
    "    shuffle=False,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "y_true = test_gen.classes\n",
    "y_pred = model.predict(test_gen, verbose=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_labels, target_names=test_gen.class_indices.keys()))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_labels)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=test_gen.class_indices.keys(),\n",
    "            yticklabels=test_gen.class_indices.keys())\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15341c67-ea99-4601-9786-408e425490cb",
   "metadata": {},
   "source": [
    "Conversi√≥n b√°sica a TFLite (para modelos basados en TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb864cb-4a09-4d73-9886-b422aafcee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Cargar modelo y tokenizador (ejemplo con T5)\n",
    "model_name = \"MobileNetV2-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Guardar modelo en formato SavedModel\n",
    "model.save_pretrained(\"./MobileNetV2_saved_model\", saved_model=True)\n",
    "\n",
    "# Convertir a TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./MobileNetV2_saved_model/saved_model/1\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Guardar modelo TFLite\n",
    "with open(\"MobileNetV2_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e5d46-9ff8-4c89-b8d4-7b2db85bb5f9",
   "metadata": {},
   "source": [
    "Exportar a TensorFlow‚ÄØLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96aa0d6-7dce-4744-bf95-a6409a3621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"plant_disease.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60055f4c-6440-4a90-933b-57a6402ddb1e",
   "metadata": {},
   "source": [
    "Conversi√≥n avanzada con cuantizaci√≥n (para reducir tama√±o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfc618-509c-4d9e-949a-34df243a5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar conversor con cuantizaci√≥n\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./MobileNetV2_saved_model/saved_model/1\")\n",
    "\n",
    "# Cuantizaci√≥n din√°mica (balance entre tama√±o/calidad)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Opcional: Especificar representantes para cuantizaci√≥n completa\n",
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        yield [tf.random.uniform(shape=(1, 128), dtype=tf.int32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(\"MobileNetV2_model_quant.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde80d8e-6227-4f3f-ad8c-7a2d31b16262",
   "metadata": {},
   "source": [
    "Alternativas recomendadas Para tu aplicaci√≥n de portfolio, considera:\n",
    "Opci√≥n A: Usar Hugging Face Pipelines con API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf9536-931f-45d2-8cc4-1e87251791e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='MobileNetV2', device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ebd92-bae6-4e72-8784-318f64f1f2ca",
   "metadata": {},
   "source": [
    "Opci√≥n B: Optimizaci√≥n con ONNX Runtime (mejor soporte para NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414440e1-2e4c-4147-bef8-d47f51b8c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install optimum[onnxruntime]\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\"MobileNetV2-small\", from_transformers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6707d3-e39a-4cf4-b2ad-d795ee83cb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f26ada-0fc3-438e-80fa-91c918e65aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77596e3-0245-402d-8c12-0314cf5ce737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c88e92-a62c-49b2-a404-0251663d0309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
